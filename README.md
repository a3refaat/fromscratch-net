# FromScratch-Net ðŸ§ 

A fully functional **multi-layer perceptron (MLP)** implemented from scratch in NumPy â€” complete with forward propagation, backpropagation, batch training, weight initialization, and support for classification tasks like MNIST.

---

##  Features

- Layered architecture: Input, Hidden, Output
- Activation functions: ReLU, Sigmoid, Softmax
- Loss functions: Cross-Entropy, MSE
- Batch training with gradient descent
- Modular design (clean, extensible Python classes)
- Glorot and He initialization
- Recursive backpropagation
- MNIST demo included
